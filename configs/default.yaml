model: "mistral-7b"
embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
embedding_model_path: "<YOUR_LOCAL_EMBEDDING_MODEL_PATH>"  # optional local cache path
faiss_index_params:
  nlist: 100
  nprobe: 10
retrieval:
  behavior_mode: balanced
  top_k: 5
  use_knowledge_base: true
  min_score: 0.28
  pipeline_guard: true
  show_citations: false
  max_context_docs: 3
  max_chars_per_doc: 1200
  focus_top_when_strong: true
conversation:
  history_length: 5
  persist_path: "data/history/session.json"
generation:
  backend: llama-cpp
  model_path: "<YOUR_LOCAL_GGUF_PATH>"
  n_ctx: 8192
  n_threads: 8
  n_gpu_layers: 32
  max_new_tokens: 256
  temperature: 0.3
  do_sample: false
  top_p: 0.9
  top_k: 40
  repeat_penalty: 1.1
  stop:
    - "\n\nContext:"
    - "\n\nQ:"
    - "\n\nQuestion:"
  output_guard: false
evaluation:
  retrieval_metrics:
    precision_at_k: [1, 5, 10]
    recall_at_k: [1, 5, 10]
  generation_metrics:
    bleu: true
    rouge: true
    factuality_check: true
logging:
  level: "INFO"
  log_file: "logs/system.log"
semantic_cache:
  max_items: 2000
  threshold: 0.92
  reuse_answer: true
